{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load environment variables\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Run basic query with OpenAI wrapper\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "llm(\"explain large language models in one sentence\")\n",
    "\n",
    "# import schema for chat messages and ChatOpenAI in order to query chatmodels GPT-3.5-turbo or GPT-4\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert data scientist\"),\n",
    "    HumanMessage(content=\"Write a Python script that trains a neural network on simulated data \")\n",
    "]\n",
    "response = chat(messages)\n",
    "\n",
    "print(response.content, end='\\n')\n",
    "\n",
    "# Import prompt and define PromptTemplate\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert data scientist with an expertise in building deep learning models. \n",
    "Explain the concept of {concept} in a couple of lines\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"concept\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "# Run LLM with PromptTemplate\n",
    "\n",
    "llm(prompt.format(concept=\"autoencoder\"))\n",
    "\n",
    "# Import LLMChain and define chain with language model and prompt as arguments.\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(\"autoencoder\"))\n",
    "\n",
    "# Define a second prompt \n",
    "\n",
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"ml_concept\"],\n",
    "    template=\"Turn the concept description of {ml_concept} and explain it to me like I'm five in 500 words\",\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "# Define a sequential chain using the two chains above: the second chain takes the output of the first chain as input\n",
    "\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "# Run the chain specifying only the input variable for the first chain.\n",
    "explanation = overall_chain.run(\"autoencoder\")\n",
    "print(explanation)\n",
    "\n",
    "# Import utility for splitting up texts and split up the explanation given above into document chunks\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([explanation])\n",
    "\n",
    "# Individual text chunks can be accessed with \"page_content\"\n",
    "\n",
    "texts[0].page_content\n",
    "\n",
    "# Import and instantiate OpenAI embeddings\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model_name=\"ada\")\n",
    "\n",
    "# Turn the first text chunk into a vector with the embedding\n",
    "\n",
    "query_result = embeddings.embed_query(texts[0].page_content)\n",
    "print(query_result)\n",
    "\n",
    "# Import and initialize Pinecone client\n",
    "\n",
    "import os\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=os.getenv('PINECONE_API_KEY'),\n",
    "    environment=os.getenv('PINECONE_ENV')\n",
    ")\n",
    "\n",
    "# Upload vectors to Pinecone\n",
    "\n",
    "index_name = \"langchain-quickstart\"\n",
    "search = Pinecone.from_documents(texts, embeddings, index_name=index_name)\n",
    "\n",
    "# Do a simple vector similarity search\n",
    "\n",
    "query = \"What is magical about an autoencoder?\"\n",
    "result = search.similarity_search(query)\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Import Python REPL tool and instantiate Python agent\n",
    "\n",
    "from langchain.agents.agent_toolkits import create_python_agent\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "from langchain.python import PythonREPL\n",
    "from langchain.llms.openai import OpenAI\n",
    "\n",
    "agent_executor = create_python_agent(\n",
    "    llm=OpenAI(temperature=0, max_tokens=1000),\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Execute the Python agent\n",
    "\n",
    "agent_executor.run(\"Find the roots (zeros) if the quadratic function 3 * x**2 + 2*x -1\")\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
